{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "礙於不是所有同學都有 GPU ，這邊的範例使用的是簡化版本的 ResNet，確保所有同學都能夠順利訓練!\n",
    "\n",
    "\n",
    "最後一天的作業請閱讀這篇非常詳盡的[文章](https://blog.gtwang.org/programming/keras-resnet-50-pre-trained-model-build-dogs-cats-image-classification-system/)，基本上已經涵蓋了所有訓練　CNN 常用的技巧，請使用所有學過的訓練技巧，盡可能地提高 Cifar-10 的 test data 準確率，截圖你最佳的結果並上傳來完成最後一次的作業吧!\n",
    "\n",
    "另外這些技巧在 Kaggle 上也會被許多人使用，更有人會開發一些新的技巧，例如使把預訓練在 ImageNet 上的模型當成 feature extractor 後，再拿擷取出的特徵重新訓練新的模型，這些技巧再進階的課程我們會在提到，有興趣的同學也可以[參考](https://www.kaggle.com/insaff/img-feature-extraction-with-pretrained-resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tensorflow-gpu in d:\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.12)\n",
      "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard~=2.5 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.5.0)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.4.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio~=1.34.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.34.1)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: wheel~=0.35 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.36.2)\n",
      "Requirement already satisfied, skipping upgrade: six~=1.15.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.6.0,>=2.5.0rc0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.5.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-nightly~=2.5.0.dev in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied, skipping upgrade: h5py~=3.1.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.15.6)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.12.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in d:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.7.0,>=0.6.0 in d:\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow-gpu) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in d:\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow-gpu) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in d:\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow-gpu) (3.3.4)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in d:\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow-gpu) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in d:\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow-gpu) (1.27.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in d:\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow-gpu) (0.4.3)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in d:\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow-gpu) (41.4.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in d:\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow-gpu) (1.8.0)\n",
      "Requirement already satisfied, skipping upgrade: cached-property; python_version < \"3.8\" in d:\\anaconda3\\lib\\site-packages (from h5py~=3.1.0->tensorflow-gpu) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in d:\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow-gpu) (3.7.3)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in d:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in d:\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (4.7.2)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in d:\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in d:\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (4.2.1)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in d:\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in d:\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow-gpu) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in d:\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in d:\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in d:\\anaconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow-gpu) (7.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow-gpu --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13943869600124854922\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4848943104\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16492749933872940307\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "y_train = keras.utils.to_categorical(y_train,10)\n",
    "y_test = keras.utils.to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(inputs,num_filters=16,\n",
    "                  kernel_size=3,strides=1,\n",
    "                  activation='relu'):\n",
    "    x = Conv2D(num_filters,kernel_size=kernel_size,strides=strides,padding='same',\n",
    "           kernel_initializer='he_normal',kernel_regularizer=l2(1e-4))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    if(activation):\n",
    "        x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1,xshape: (None, 32, 32, 16)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_2[0][0]               \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_4[0][0]               \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 16)   2320        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_6[0][0]               \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 16)   2320        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 16)   0           activation_8[0][0]               \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 16)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 16)   2320        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 16)   64          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 16)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 16)   2320        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 16)   64          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 16)   0           activation_10[0][0]              \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 16)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   4640        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 32)   4640        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 32)   0           conv2d_15[0][0]                  \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 32)   9248        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 32)   9248        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 16, 32)   0           activation_14[0][0]              \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 32)   0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 32)   128         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 32)   9248        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 32)   128         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 32)   0           activation_16[0][0]              \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 32)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 32)   9248        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 32)   128         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 32)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 32)   9248        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 32)   128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 16, 16, 32)   0           activation_18[0][0]              \n",
      "                                                                 batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 32)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 32)   9248        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 32)   128         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 32)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 32)   9248        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 32)   128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 16, 16, 32)   0           activation_20[0][0]              \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 32)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 32)   9248        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 32)   128         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 32)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 32)   9248        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 32)   128         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 16, 16, 32)   0           activation_22[0][0]              \n",
      "                                                                 batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 32)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 64)     18496       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 8, 8, 64)     256         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 8, 64)     0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 64)     36928       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 64)     18496       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 8, 8, 64)     256         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 8, 8, 64)     0           conv2d_28[0][0]                  \n",
      "                                                                 batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 64)     0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 64)     36928       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 64)     256         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 64)     0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 64)     36928       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 64)     256         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 8, 8, 64)     0           activation_26[0][0]              \n",
      "                                                                 batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 64)     0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 64)     36928       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 8, 8, 64)     256         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 64)     0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 8, 8, 64)     36928       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 64)     256         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 8, 8, 64)     0           activation_28[0][0]              \n",
      "                                                                 batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 64)     0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 8, 8, 64)     36928       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 8, 8, 64)     256         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 8, 64)     0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 8, 8, 64)     36928       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 8, 8, 64)     256         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 8, 8, 64)     0           activation_30[0][0]              \n",
      "                                                                 batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 8, 8, 64)     0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 8, 8, 64)     36928       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 8, 8, 64)     256         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 8, 8, 64)     0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 8, 64)     36928       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 8, 8, 64)     256         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 8, 8, 64)     0           activation_32[0][0]              \n",
      "                                                                 batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 8, 64)     0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 8, 64)     36928       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 8, 8, 64)     256         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 64)     0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 64)     36928       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 8, 8, 64)     256         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 8, 8, 64)     0           activation_34[0][0]              \n",
      "                                                                 batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 64)     0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 4, 4, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1024)         0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           10250       flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 598,186\n",
      "Trainable params: 595,466\n",
      "Non-trainable params: 2,720\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def resnet_v1(input_shape):\n",
    "    inputs = Input(shape=input_shape)# Input层，用来当做占位使用\n",
    "    \n",
    "    #第一层\n",
    "    x = resnet_block(inputs)\n",
    "    print('layer1,xshape:',x.shape)\n",
    "    # 第2~7层\n",
    "    for i in range(6):\n",
    "        a = resnet_block(inputs = x)\n",
    "        b = resnet_block(inputs=a,activation=None)\n",
    "        x = keras.layers.add([x,b])\n",
    "        x = Activation('relu')(x)\n",
    "    # out：32*32*16\n",
    "    # 第8~13层\n",
    "    for i in range(6):\n",
    "        if i == 0:\n",
    "            a = resnet_block(inputs = x,strides=2,num_filters=32)\n",
    "        else:\n",
    "            a = resnet_block(inputs = x,num_filters=32)\n",
    "        b = resnet_block(inputs=a,activation=None,num_filters=32)\n",
    "        if i==0:\n",
    "            x = Conv2D(32,kernel_size=3,strides=2,padding='same',\n",
    "                       kernel_initializer='he_normal',kernel_regularizer=l2(1e-4))(x)\n",
    "        x = keras.layers.add([x,b])\n",
    "        x = Activation('relu')(x)\n",
    "    # out:16*16*32\n",
    "    # 第14~19层\n",
    "    for i in range(6):\n",
    "        if i ==0 :\n",
    "            a = resnet_block(inputs = x,strides=2,num_filters=64)\n",
    "        else:\n",
    "            a = resnet_block(inputs = x,num_filters=64)\n",
    "\n",
    "        b = resnet_block(inputs=a,activation=None,num_filters=64)\n",
    "        if i == 0:\n",
    "            x = Conv2D(64,kernel_size=3,strides=2,padding='same',\n",
    "                       kernel_initializer='he_normal',kernel_regularizer=l2(1e-4))(x)\n",
    "        x = keras.layers.add([x,b])# 相加操作，要求x、b shape完全一致\n",
    "        x = Activation('relu')(x)\n",
    "    # out:8*8*64\n",
    "    # 第20层   \n",
    "    x = AveragePooling2D(pool_size=2)(x)\n",
    "    # out:4*4*64\n",
    "    y = Flatten()(x)\n",
    "    # out:1024\n",
    "    outputs = Dense(10,activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "    \n",
    "    #初始化模型\n",
    "    #之前的操作只是将多个神经网络层进行了相连，通过下面这一句的初始化操作，才算真正完成了一个模型的结构初始化\n",
    "    model = Model(inputs=inputs,outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = resnet_v1((32,32,3))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=Adam(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='./cifar10_resnet_ckpt.h5',monitor='val_accuracy',\n",
    "                             verbose=1,save_best_only=True)\n",
    "def lr_sch(epoch):\n",
    "    #200 total\n",
    "    if epoch <50:\n",
    "        return 1e-3\n",
    "    if 50<=epoch<100:\n",
    "        return 1e-4\n",
    "    if epoch>=100:\n",
    "        return 1e-5\n",
    "lr_scheduler = LearningRateScheduler(lr_sch)\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_accuracy',factor=0.2,patience=5,\n",
    "                               mode='max',min_lr=1e-3)\n",
    "callbacks = [checkpoint,lr_scheduler,lr_reducer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 1.1869 - accuracy: 0.6632 - val_loss: 1.4824 - val_accuracy: 0.6026\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.60260, saving model to .\\cifar10_resnet_ckpt.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 1.1217 - accuracy: 0.6804 - val_loss: 1.5873 - val_accuracy: 0.5960\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.60260\n",
      "Epoch 3/100\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 1.0815 - accuracy: 0.6947 - val_loss: 1.7139 - val_accuracy: 0.5624\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.60260\n",
      "Epoch 4/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 1.0229 - accuracy: 0.7143 - val_loss: 1.0121 - val_accuracy: 0.7241\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.60260 to 0.72410, saving model to .\\cifar10_resnet_ckpt.h5\n",
      "Epoch 5/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.9827 - accuracy: 0.7295 - val_loss: 1.3500 - val_accuracy: 0.6441\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.72410\n",
      "Epoch 6/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.9393 - accuracy: 0.7419 - val_loss: 1.0425 - val_accuracy: 0.7206\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.72410\n",
      "Epoch 7/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.9130 - accuracy: 0.7511 - val_loss: 1.2503 - val_accuracy: 0.6624\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.72410\n",
      "Epoch 8/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.8793 - accuracy: 0.7642 - val_loss: 1.1097 - val_accuracy: 0.6974\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.72410\n",
      "Epoch 9/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.8483 - accuracy: 0.7726 - val_loss: 0.8727 - val_accuracy: 0.7675\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.72410 to 0.76750, saving model to .\\cifar10_resnet_ckpt.h5\n",
      "Epoch 10/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.8245 - accuracy: 0.7790 - val_loss: 1.1340 - val_accuracy: 0.7079\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.76750\n",
      "Epoch 11/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.8083 - accuracy: 0.7852 - val_loss: 0.9233 - val_accuracy: 0.7572\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.76750\n",
      "Epoch 12/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.7864 - accuracy: 0.7916 - val_loss: 1.0103 - val_accuracy: 0.7370\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.76750\n",
      "Epoch 13/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.7672 - accuracy: 0.7998 - val_loss: 1.0262 - val_accuracy: 0.7472\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.76750\n",
      "Epoch 14/100\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.7572 - accuracy: 0.8016 - val_loss: 0.9203 - val_accuracy: 0.7591\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.76750\n",
      "Epoch 15/100\n",
      "390/390 [==============================] - 41s 104ms/step - loss: 0.7371 - accuracy: 0.8095 - val_loss: 0.9237 - val_accuracy: 0.7584\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.76750\n",
      "Epoch 16/100\n",
      "390/390 [==============================] - 41s 105ms/step - loss: 0.7226 - accuracy: 0.8130 - val_loss: 1.0182 - val_accuracy: 0.7529\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.76750\n",
      "Epoch 17/100\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.7127 - accuracy: 0.8160 - val_loss: 0.8364 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.76750 to 0.79000, saving model to .\\cifar10_resnet_ckpt.h5\n",
      "Epoch 18/100\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.6969 - accuracy: 0.8220 - val_loss: 0.9697 - val_accuracy: 0.7487\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.79000\n",
      "Epoch 19/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.6886 - accuracy: 0.8248 - val_loss: 0.8059 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.79000 to 0.79650, saving model to .\\cifar10_resnet_ckpt.h5\n",
      "Epoch 20/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.6829 - accuracy: 0.8282 - val_loss: 0.7459 - val_accuracy: 0.8105\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.79650 to 0.81050, saving model to .\\cifar10_resnet_ckpt.h5\n",
      "Epoch 21/100\n",
      "390/390 [==============================] - 41s 104ms/step - loss: 0.6681 - accuracy: 0.8329 - val_loss: 1.2001 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.81050\n",
      "Epoch 22/100\n",
      "390/390 [==============================] - 41s 105ms/step - loss: 0.6598 - accuracy: 0.8331 - val_loss: 0.8505 - val_accuracy: 0.7951\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.81050\n",
      "Epoch 23/100\n",
      "390/390 [==============================] - 41s 105ms/step - loss: 0.6525 - accuracy: 0.8369 - val_loss: 0.8174 - val_accuracy: 0.7830\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.81050\n",
      "Epoch 24/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.6456 - accuracy: 0.8416 - val_loss: 0.8567 - val_accuracy: 0.7881\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.81050\n",
      "Epoch 25/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.6382 - accuracy: 0.8429 - val_loss: 0.6946 - val_accuracy: 0.8274\n",
      "\n",
      "Epoch 00025: val_accuracy improved from 0.81050 to 0.82740, saving model to .\\cifar10_resnet_ckpt.h5\n",
      "Epoch 26/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.6231 - accuracy: 0.8459 - val_loss: 0.9403 - val_accuracy: 0.7729\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.82740\n",
      "Epoch 27/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.6228 - accuracy: 0.8471 - val_loss: 0.7712 - val_accuracy: 0.8110\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.82740\n",
      "Epoch 28/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.6183 - accuracy: 0.8487 - val_loss: 0.9207 - val_accuracy: 0.7829\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.82740\n",
      "Epoch 29/100\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.6054 - accuracy: 0.8542 - val_loss: 0.8556 - val_accuracy: 0.7860\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.82740\n",
      "Epoch 30/100\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.6043 - accuracy: 0.8548 - val_loss: 0.8234 - val_accuracy: 0.7925\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.82740\n",
      "Epoch 31/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.6050 - accuracy: 0.8530 - val_loss: 0.7816 - val_accuracy: 0.8117\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.82740\n",
      "Epoch 32/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.5956 - accuracy: 0.8573 - val_loss: 0.7052 - val_accuracy: 0.8330\n",
      "\n",
      "Epoch 00032: val_accuracy improved from 0.82740 to 0.83300, saving model to .\\cifar10_resnet_ckpt.h5\n",
      "Epoch 33/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.5903 - accuracy: 0.8599 - val_loss: 0.8559 - val_accuracy: 0.8045\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.83300\n",
      "Epoch 34/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.5864 - accuracy: 0.8587 - val_loss: 0.7194 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.83300\n",
      "Epoch 35/100\n",
      "390/390 [==============================] - 41s 105ms/step - loss: 0.5847 - accuracy: 0.8593 - val_loss: 0.7543 - val_accuracy: 0.8222\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.83300\n",
      "Epoch 36/100\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.5782 - accuracy: 0.8647 - val_loss: 0.7550 - val_accuracy: 0.8227\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.83300\n",
      "Epoch 37/100\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.5699 - accuracy: 0.8655 - val_loss: 0.8257 - val_accuracy: 0.8019\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.83300\n",
      "Epoch 38/100\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.5637 - accuracy: 0.8681 - val_loss: 0.8345 - val_accuracy: 0.8088\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.83300\n",
      "Epoch 39/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.5632 - accuracy: 0.8683 - val_loss: 0.6186 - val_accuracy: 0.8543\n",
      "\n",
      "Epoch 00039: val_accuracy improved from 0.83300 to 0.85430, saving model to .\\cifar10_resnet_ckpt.h5\n",
      "Epoch 40/100\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.5627 - accuracy: 0.8682 - val_loss: 0.7041 - val_accuracy: 0.8330\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.85430\n",
      "Epoch 41/100\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.5550 - accuracy: 0.8703 - val_loss: 0.7969 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.85430\n",
      "Epoch 42/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.5513 - accuracy: 0.8727 - val_loss: 0.7782 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.85430\n",
      "Epoch 43/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.5522 - accuracy: 0.8720 - val_loss: 0.7436 - val_accuracy: 0.8164\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.85430\n",
      "Epoch 44/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.5440 - accuracy: 0.8747 - val_loss: 0.7768 - val_accuracy: 0.8140\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.85430\n",
      "Epoch 45/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.5407 - accuracy: 0.8735 - val_loss: 0.6980 - val_accuracy: 0.8352\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.85430\n",
      "Epoch 46/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.5430 - accuracy: 0.8749 - val_loss: 0.7148 - val_accuracy: 0.8298\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.85430\n",
      "Epoch 47/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.5439 - accuracy: 0.8741 - val_loss: 0.7165 - val_accuracy: 0.8364\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.85430\n",
      "Epoch 48/100\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.5399 - accuracy: 0.8756 - val_loss: 0.7510 - val_accuracy: 0.8317\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.85430\n",
      "Epoch 49/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.5355 - accuracy: 0.8788 - val_loss: 0.8028 - val_accuracy: 0.8141\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.85430\n",
      "Epoch 50/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.5346 - accuracy: 0.8768 - val_loss: 0.6434 - val_accuracy: 0.8501\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.85430\n",
      "Epoch 51/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.4552 - accuracy: 0.9053 - val_loss: 0.5157 - val_accuracy: 0.8929\n",
      "\n",
      "Epoch 00051: val_accuracy improved from 0.85430 to 0.89290, saving model to .\\cifar10_resnet_ckpt.h5\n",
      "Epoch 52/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.4283 - accuracy: 0.9137 - val_loss: 0.5110 - val_accuracy: 0.8920\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.89290\n",
      "Epoch 53/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.4166 - accuracy: 0.9172 - val_loss: 0.4987 - val_accuracy: 0.8998\n",
      "\n",
      "Epoch 00053: val_accuracy improved from 0.89290 to 0.89980, saving model to .\\cifar10_resnet_ckpt.h5\n",
      "Epoch 54/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.4039 - accuracy: 0.9210 - val_loss: 0.5151 - val_accuracy: 0.8965\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.89980\n",
      "Epoch 55/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.3934 - accuracy: 0.9240 - val_loss: 0.4881 - val_accuracy: 0.9035\n",
      "\n",
      "Epoch 00055: val_accuracy improved from 0.89980 to 0.90350, saving model to .\\cifar10_resnet_ckpt.h5\n",
      "Epoch 56/100\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.3929 - accuracy: 0.9245 - val_loss: 0.5109 - val_accuracy: 0.8963\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.90350\n",
      "Epoch 57/100\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.3822 - accuracy: 0.9262 - val_loss: 0.4864 - val_accuracy: 0.9042\n",
      "\n",
      "Epoch 00057: val_accuracy improved from 0.90350 to 0.90420, saving model to .\\cifar10_resnet_ckpt.h5\n",
      "Epoch 58/100\n",
      "390/390 [==============================] - 40s 104ms/step - loss: 0.3843 - accuracy: 0.9261 - val_loss: 0.5000 - val_accuracy: 0.8987\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.90420\n",
      "Epoch 59/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.3742 - accuracy: 0.9284 - val_loss: 0.5069 - val_accuracy: 0.9002\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.90420\n",
      "Epoch 60/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.3724 - accuracy: 0.9302 - val_loss: 0.4971 - val_accuracy: 0.8997\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.90420\n",
      "Epoch 61/100\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.3687 - accuracy: 0.9291 - val_loss: 0.5022 - val_accuracy: 0.8987\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.90420\n",
      "Epoch 62/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.3604 - accuracy: 0.9335 - val_loss: 0.4932 - val_accuracy: 0.9026\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.90420\n",
      "Epoch 63/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.3566 - accuracy: 0.9328 - val_loss: 0.5103 - val_accuracy: 0.9001\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.90420\n",
      "Epoch 64/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.3545 - accuracy: 0.9333 - val_loss: 0.4861 - val_accuracy: 0.9034\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.90420\n",
      "Epoch 65/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.3533 - accuracy: 0.9328 - val_loss: 0.5297 - val_accuracy: 0.8928\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.90420\n",
      "Epoch 66/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.3486 - accuracy: 0.9351 - val_loss: 0.4900 - val_accuracy: 0.9023\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.90420\n",
      "Epoch 67/100\n",
      "390/390 [==============================] - 40s 104ms/step - loss: 0.3435 - accuracy: 0.9355 - val_loss: 0.4954 - val_accuracy: 0.9030\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.90420\n",
      "Epoch 68/100\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.3415 - accuracy: 0.9367 - val_loss: 0.5047 - val_accuracy: 0.8990\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.90420\n",
      "Epoch 69/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.3381 - accuracy: 0.9368 - val_loss: 0.5064 - val_accuracy: 0.8996\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.90420\n",
      "Epoch 70/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.3341 - accuracy: 0.9387 - val_loss: 0.4967 - val_accuracy: 0.8991\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.90420\n",
      "Epoch 71/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.3334 - accuracy: 0.9391 - val_loss: 0.5015 - val_accuracy: 0.9006\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.90420\n",
      "Epoch 72/100\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.3284 - accuracy: 0.9396 - val_loss: 0.4865 - val_accuracy: 0.9044\n",
      "\n",
      "Epoch 00072: val_accuracy improved from 0.90420 to 0.90440, saving model to .\\cifar10_resnet_ckpt.h5\n",
      "Epoch 73/100\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.3273 - accuracy: 0.9402 - val_loss: 0.4901 - val_accuracy: 0.9031\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.90440\n",
      "Epoch 74/100\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.3229 - accuracy: 0.9420 - val_loss: 0.4715 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00074: val_accuracy improved from 0.90440 to 0.90730, saving model to .\\cifar10_resnet_ckpt.h5\n",
      "Epoch 75/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.3217 - accuracy: 0.9413 - val_loss: 0.5044 - val_accuracy: 0.9009\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.90730\n",
      "Epoch 76/100\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.3216 - accuracy: 0.9419 - val_loss: 0.4986 - val_accuracy: 0.9004\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.90730\n",
      "Epoch 77/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.3208 - accuracy: 0.9416 - val_loss: 0.4845 - val_accuracy: 0.9049\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.90730\n",
      "Epoch 78/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.3174 - accuracy: 0.9415 - val_loss: 0.4795 - val_accuracy: 0.9030\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.90730\n",
      "Epoch 79/100\n",
      "390/390 [==============================] - 40s 103ms/step - loss: 0.3102 - accuracy: 0.9443 - val_loss: 0.4638 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00079: val_accuracy improved from 0.90730 to 0.90990, saving model to .\\cifar10_resnet_ckpt.h5\n",
      "Epoch 80/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.3106 - accuracy: 0.9433 - val_loss: 0.4868 - val_accuracy: 0.9025\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.90990\n",
      "Epoch 81/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.3072 - accuracy: 0.9458 - val_loss: 0.4995 - val_accuracy: 0.9033\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.90990\n",
      "Epoch 82/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.3072 - accuracy: 0.9446 - val_loss: 0.4790 - val_accuracy: 0.9042\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.90990\n",
      "Epoch 83/100\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.3008 - accuracy: 0.9459 - val_loss: 0.5588 - val_accuracy: 0.8915\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.90990\n",
      "Epoch 84/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.3047 - accuracy: 0.9442 - val_loss: 0.5160 - val_accuracy: 0.8984\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.90990\n",
      "Epoch 85/100\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.3005 - accuracy: 0.9458 - val_loss: 0.4868 - val_accuracy: 0.9044\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.90990\n",
      "Epoch 86/100\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.2994 - accuracy: 0.9456 - val_loss: 0.4673 - val_accuracy: 0.9109\n",
      "\n",
      "Epoch 00086: val_accuracy improved from 0.90990 to 0.91090, saving model to .\\cifar10_resnet_ckpt.h5\n",
      "Epoch 87/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.2973 - accuracy: 0.9471 - val_loss: 0.5005 - val_accuracy: 0.9007\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.91090\n",
      "Epoch 88/100\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.2933 - accuracy: 0.9474 - val_loss: 0.5105 - val_accuracy: 0.8996\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.91090\n",
      "Epoch 89/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.2926 - accuracy: 0.9480 - val_loss: 0.4762 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.91090\n",
      "Epoch 90/100\n",
      "390/390 [==============================] - 39s 100ms/step - loss: 0.2915 - accuracy: 0.9466 - val_loss: 0.5333 - val_accuracy: 0.8941\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.91090\n",
      "Epoch 91/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.2881 - accuracy: 0.9493 - val_loss: 0.5177 - val_accuracy: 0.9008\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.91090\n",
      "Epoch 92/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.2877 - accuracy: 0.9495 - val_loss: 0.4921 - val_accuracy: 0.9034\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.91090\n",
      "Epoch 93/100\n",
      "390/390 [==============================] - 40s 101ms/step - loss: 0.2830 - accuracy: 0.9501 - val_loss: 0.4737 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.91090\n",
      "Epoch 94/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.2853 - accuracy: 0.9488 - val_loss: 0.5012 - val_accuracy: 0.9013\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.91090\n",
      "Epoch 95/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.2858 - accuracy: 0.9484 - val_loss: 0.4768 - val_accuracy: 0.9058\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.91090\n",
      "Epoch 96/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.2798 - accuracy: 0.9522 - val_loss: 0.4952 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.91090\n",
      "Epoch 97/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.2777 - accuracy: 0.9512 - val_loss: 0.4905 - val_accuracy: 0.9041\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.91090\n",
      "Epoch 98/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.2791 - accuracy: 0.9515 - val_loss: 0.4886 - val_accuracy: 0.9074\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.91090\n",
      "Epoch 99/100\n",
      "390/390 [==============================] - 39s 101ms/step - loss: 0.2746 - accuracy: 0.9532 - val_loss: 0.5094 - val_accuracy: 0.9040\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.91090\n",
      "Epoch 100/100\n",
      "390/390 [==============================] - 40s 102ms/step - loss: 0.2738 - accuracy: 0.9515 - val_loss: 0.4961 - val_accuracy: 0.9040\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.91090\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "data_generator = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "augmented_image = data_generator.flow(x_train,y_train,batch_size = batch_size)\n",
    "with tf.device('/device:GPU:0'):\n",
    "                    model.fit_generator(augmented_image,\n",
    "                   steps_per_epoch = int(len(x_train)/batch_size),\n",
    "                   verbose = 1,\n",
    "                   validation_data = (x_test,y_test),\n",
    "                   epochs = epochs,\n",
    "                   callbacks = callbacks)\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 15ms/step - loss: 0.4961 - accuracy: 0.9040\n",
      "Test loss: 0.4960649311542511\n",
      "Test accuracy: 0.9039999842643738\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test,y_test,verbose = 1)\n",
    "print('Test loss:',score[0])\n",
    "print('Test accuracy:',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
